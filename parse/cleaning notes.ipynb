{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals/approach/data used\n",
    "\n",
    "The specific datasets used here are posted here.\n",
    "\n",
    "https://docs.google.com/document/d/1Kos4MTmkEcEqaQUhtKWek8bN3aTNBr4WcDZptRvZOI8/edit \n",
    "\n",
    "I'm starting off with manually cleaning them for now and observing recurring issues that I can generalize/automate for. Some of these issues include:\n",
    "* imputation (when it makes sense, when it doesn't, what kind of imputation)\n",
    "* information formatting for datetime, location, etc\n",
    "* data that is split across columns but should be 'one' field\n",
    "* etc\n",
    "\n",
    "every dataset we have is for the most part well formed, so i haven't actually needed to test for those issues haha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import usaddress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/Behavioral_Risk_Factor_Data__Heart_Disease___Stroke_Prevention.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Year', 'LocationAbbr', 'LocationDesc', 'Datasource',\n",
       "       'PriorityArea1', 'PriorityArea2', 'PriorityArea3', 'PriorityArea4',\n",
       "       'Category', 'Topic', 'Indicator', 'Data_Value_Type',\n",
       "       'Data_Value_Unit', 'Data_Value', 'Data_Value_Alt',\n",
       "       'Data_Value_Footnote_Symbol', 'Data_Value_Footnote',\n",
       "       'Confidence_Limit_Low', 'Confidence_Limit_High',\n",
       "       'Break_Out_Category', 'Break_out', 'CategoryID', 'TopicID',\n",
       "       'IndicatorID', 'Data_Value_TypeID', 'BreakoutCategoryID',\n",
       "       'BreakOutID', 'LocationID', 'GeoLocation'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Major Cardiovascular Disease', 'Stroke', 'Diabetes',\n",
       "       'Acute Myocardial Infarction (Heart Attack)',\n",
       "       'Coronary Heart Disease', 'Cholesterol Abnormalities', 'Nutrition',\n",
       "       'Obesity', 'Smoking', 'Physical Inactivity', 'Hypertension'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Topic'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 'topics' are exactly what we want for metadata tagging; the issue is that most of this could not have been inferred from the headers alone or from the dataset description on the site. \n",
    "\n",
    "To quickly identify columns with data that is relevant for metadata tagging, I think we could build a dictionary of medical related terms (pretty trivial) and cross reference subsets of each column with this dictionary. If we find some 'good' columns, we can label and consider those for the tagging stage. If not, it's no big deal - not much overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Prevalence of major cardiovascular disease among US adults (18+); BRFSS',\n",
       "       'Prevalence of stroke among US adults (18+); BRFSS',\n",
       "       'Prevalence of diabetes among US adults (18+); BRFSS',\n",
       "       'Prevalence of acute myocardial infarction (heart attack) among US adults (18+); BRFSS',\n",
       "       'Prevalence of coronary heart disease among US adults (18+); BRFSS',\n",
       "       'Prevalence of post-hospitalization rehabilitation among heart attack patients, US adults (18+); BRFSS',\n",
       "       'Prevalence of cholesterol screening in the past 5 years among US adults (20+); BRFSS',\n",
       "       'Prevalence of high total cholesterol among US adults (20+); BRFSS',\n",
       "       'Prevalence of consuming fruits and vegetables less than 5 times per day among US adults (18+); BRFSS',\n",
       "       'Prevalence of obesity among US adults (20+); BRFSS',\n",
       "       'Prevalence of healthy weight among US adults (20+); BRFSS',\n",
       "       'Prevalence of current smoking among US adults (18+); BRFSS',\n",
       "       'Prevalence of physical inactivity among US adults (18+); BRFSS',\n",
       "       'Prevalence of hypertension among US adults (18+); BRFSS',\n",
       "       'Prevalence of hypertension medication use among US adults (18+) with hypertension; BRFSS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Indicator'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only way to tell what the Data Values correspond to is to look at the corresponding 'indicator' value rather than the column header. This is kind of dumb and I can hope that future csvs don't have this feature. We might want to 'unpack' the datasets (e.g. each 'indicator' should be its own datafile and represent a header rather than a column entry) so that each of these indicators are independent datasets/tagged independently (e.g. data about 'consuming fruits and vegetables...' would be a SEPARATE dataset tagged with food+heart disease, data about 'physical inactivity' would be a separate dataset tagged for fitness, etc. it wouldn't make sense to tag all these separate things the same, would it?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Detecting what constitutes invalid 'null' data\n",
    "\n",
    "From this/the other datasets posted, there honestly doesn't seem like a huge issue with just straight pandas->read_csv->dropna(subset) based on a manual selection of a subset of columns to pay attention to. That's because all the datasets are already well-formed though.\n",
    "\n",
    "For this dataframe, it appears just testing for missing entries is totally fine. The question is what columns to test this on; testing on all columns returns more 'missing' data than need be. For example if we dropped rows based on NaN in the 'Data Value Footnote' field for example we'd drop way too much. How to intelligently tell what columns to base this decision on? It appears that we need to manually identify columns that 'matter' first (e.g the footnote doesn't matter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 'Year'],\n",
       " [0, 'LocationAbbr'],\n",
       " [0, 'LocationDesc'],\n",
       " [0, 'Datasource'],\n",
       " [0, 'PriorityArea1'],\n",
       " [0, 'PriorityArea2'],\n",
       " [0, 'PriorityArea3'],\n",
       " [0, 'PriorityArea4'],\n",
       " [0, 'Category'],\n",
       " [0, 'Topic'],\n",
       " [0, 'Indicator'],\n",
       " [0, 'Data_Value_Type'],\n",
       " [0, 'Data_Value_Unit'],\n",
       " [0, 'Data_Value'],\n",
       " [0, 'Data_Value_Alt'],\n",
       " [57032, 'Data_Value_Footnote_Symbol'],\n",
       " [57032, 'Data_Value_Footnote'],\n",
       " [1399, 'Confidence_Limit_Low'],\n",
       " [1399, 'Confidence_Limit_High'],\n",
       " [0, 'Break_Out_Category'],\n",
       " [0, 'Break_out'],\n",
       " [0, 'CategoryID'],\n",
       " [0, 'TopicID'],\n",
       " [0, 'IndicatorID'],\n",
       " [0, 'Data_Value_TypeID'],\n",
       " [0, 'BreakoutCategoryID'],\n",
       " [0, 'BreakOutID'],\n",
       " [0, 'LocationID'],\n",
       " [1399, 'GeoLocation']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[((~df['Data_Value'].isnull()) & (df[col].isnull())).sum(), col] for col in df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another point that comes up; this dataset is fucking stupid and has 1399 rows that are already aggregates of the data (the medians the data, but just re-entered as rows, and denoted in the 'Location' field). This could mess up analysis and should be cleaned. Hopefully the data we get isn't that dumb; how to systematically test that? I'll leave it alone for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Connecticut', 'Delaware', 'Median of all states',\n",
       "       'Washington, DC', 'Florida', 'Georgia', 'Hawaii', 'Idaho',\n",
       "       'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n",
       "       'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
       "       'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada',\n",
       "       'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n",
       "       'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n",
       "       'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
       "       'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LocationDesc'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Data Imputation \n",
    "\n",
    "Is this the data scientists' problem, or our problem? Different imputation methods introduce different biases/is dependent on the nature of the data itself. Do we want to test for whether data is missing approx at random (and just dropping it won't matter much) or what? idk how this is handled typically in ml/data sci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Manually shaping the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a mapping of desired column names to ordered columns in the dataset and joins that data together.\n",
    "#Used to combine data split across different columns (e.g. in the in the 'crimelab' dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifies column data that would be useful for classification purposes based on a corpus of medical text. \n",
    "# E.g. the 'Indicator' column in the heart disease data, or the 'combined OD1 ... combined odX' data in the crimelab data\n",
    "!pip install pymedtermino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_relevant_for_tagging(df):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
