{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This project will capture the end to end flow from receiving documents till query time of these documents. That is, we'll be dealing with the parsing, storing, and clustering of our database.\n",
    "\n",
    "On a high level, documents of various formats will be provided accompanied by some metadata (i.e. a paper or etc.). This will go to our parsers and then be formatted into a JSON document that will be uploaded on to an elasticsearch cluster. We will then train an [LDA Model](https://radimrehurek.com/gensim/models/ldamodel.html) to obtain latent topics based off the the accompanied metadata. On query time, given a list of descriptors, we will then try to find the relevant topics associated with these descriptors. \n",
    "\n",
    "Prior to running this notebook, run the following on terminal:\n",
    "```\n",
    "conda install elasticsearch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring Datasets\n",
    "\n",
    "We begin by obtaining data sets and storing it in a [JSON list](https://docs.google.com/document/d/1gSiucl9H1AR-2aCdE4dvOPwHZxP6zpkFzpIRfJ0usDc/edit). `Filename` represents the name of the file that was downloaded, `Data Source` represents the URL where we will manually download the file from, and `Relevant Articles` is what we'll use for our corpus to train our LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = [\n",
    "  {\n",
    "    \"Filename\": \"U.S._Chronic_Disease_Indicators__CDI_.csv\",\n",
    "    \"Data Source\": \"https://chronicdata.cdc.gov/views/g4ie-h725/rows.csv?accessType=DOWNLOAD\",\n",
    "    \"Relevant Articles\": [\n",
    "      \"https://www.cdc.gov/mmwr/pdf/rr/rr6401.pdf\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"Filename\": \"diabetic_data.csv\",\n",
    "    \"Data Source\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/00296/\",\n",
    "    \"Relevant Articles\": [\n",
    "      \"https://www.hindawi.com/journals/bmri/2014/781670/\",\n",
    "      \"https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"Filename\": \"2016_Central_Line-Associated_Bloodstream_Infections__CLABSI__Table__Original_Baseline_.csv\",\n",
    "    \"Data Source\": \"https://data.oregon.gov/api/views/757s-zskx/rows.csv?accessType=DOWNLOAD\",\n",
    "    \"Relevant Articles\": [\n",
    "      \"https://www.oregon.gov/oha/PH/DISEASESCONDITIONS/COMMUNICABLEDISEASE/HAI/Documents/Reports/2016_HAI_Annual_Report.pdf\",\n",
    "      \"https://www.oregon.gov/oha/PH/DISEASESCONDITIONS/COMMUNICABLEDISEASE/HAI/Documents/Reports/2016_HAI_Annual_Report_Exec_Summary.pdf\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"Filename\": \"crimelabaccidentaldrugdeathsextract2017.csv\",\n",
    "    \"Data Source\": \"https://data.wprdc.org/dataset/7fb0505e-8e2c-4825-b22c-4fbee8fc8010/resource/2d963e35-4f69-495e-985e-55acd72c87ca/download/crimelabaccidentaldrugdeathsextract2017.csv\",\n",
    "    \"Relevant Articles\": [\n",
    "      \"https://www.alleghenycountyanalytics.us/wp-content/uploads/2017/04/Opiate-Related-Overdose-Deaths-in-Allegheny-County.pdf\",\n",
    "      \"https://data.wprdc.org/dataset/7fb0505e-8e2c-4825-b22c-4fbee8fc8010/resource/a71e43e1-5a38-4fb3-b5f8-6ed7e51caade/download/me-data-dictionary.pdf\",\n",
    "      \"https://www.overdosefreepa.pitt.edu/know-the-facts/view-overdose-death-data/\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"Filename\": \"EMS_-_Transport_Count_by_Destination.csv\",\n",
    "    \"Data Source\": \"https://data.austintexas.gov/api/views/jtkc-5pgh/rows.csv?accessType=DOWNLOAD\",\n",
    "    \"Relevant Articles\": [\n",
    "      \"https://data.austintexas.gov/Public-Safety/EMS-Transport-Count-by-Destination/jtkc-5pgh\",\n",
    "      \"https://data.austintexas.gov/api/views/jtkc-5pgh/files/l9hg5sYLDEzQMylruCIknoVFpSq9kwMX2RvmlqN51g4?download=true&filename=EMS%20-%20Transport%20Count%20by%20Destination%20Metadata.pdf\",\n",
    "      \"http://www.austintexas.gov/department/ems\"\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Uploading to Elasticsearch\n",
    "The datasets above are all in csv format for easy parsing. When expanding our use case, we will use various types of data.\n",
    "\n",
    "### DO NOT RUN THE FOLLOWING AGAIN. EACH DOCUMENT IS GIVEN A UNIQUE ID AND WILL NOT OVERRIDE IT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading documents...\n",
      "(2227, [])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "def convert_csv_to_json(csv_file):\n",
    "    with open(csv_file) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = list(reader)\n",
    "    return (rows)\n",
    "\n",
    "def format_es_document(index, document):\n",
    "    return {\n",
    "        \"_index\": index.lower(), # Must be lowercase\n",
    "        \"_type\": index.lower(),\n",
    "        \"_source\": document\n",
    "    }\n",
    "\n",
    "es = Elasticsearch(\"https://search-data-pipeline-poc-bdfr3wal5lxncg2zllpoo6nd2e.us-east-1.es.amazonaws.com\")\n",
    "es_documents_for_datasets = []\n",
    "for data in data_sets:\n",
    "    raw_documents = convert_csv_to_json(\"datasets/\" + data[\"Filename\"])\n",
    "    for raw_document in raw_documents:\n",
    "        es_documents_for_datasets.append(format_es_document(data[\"Filename\"], raw_document))\n",
    "    \n",
    "print(\"Uploading documents...\")\n",
    "#     res = es.index(index=data[\"Filename\"],doc_type='post', body=document)\n",
    "#     print(res['result'])\n",
    "print(helpers.bulk(es, es_documents_for_datasets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving Corpuses for LDA Model and Model Generation\n",
    "This section will derive latent topics given the the articles specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim import corpora\n",
    "from StringIO import StringIO\n",
    "\n",
    "import PyPDF2\n",
    "import string\n",
    "import re\n",
    "import urllib\n",
    "import gensim\n",
    "\n",
    "def combine_relevant_articles(data):\n",
    "    joined_articles = \"\"\n",
    "    for link in data[\"Relevant Articles\"]:\n",
    "        if \"pdf\" in link:\n",
    "            file = urllib.urlopen(link).read()\n",
    "            pdf = PyPDF2.PdfFileReader(StringIO(file))\n",
    "            for i in range(pdf.getNumPages()):\n",
    "                joined_articles += pdf.getPage(i).extractText()\n",
    "        else:\n",
    "            file = urllib.urlopen(link)\n",
    "            document = file.read()\n",
    "            joined_articles += \"\".join([text for text in BeautifulSoup(document).findAll(text=True)])\n",
    "    return joined_articles\n",
    "        \n",
    "def derive_corpuses(data_sets):\n",
    "    documents = []\n",
    "    for data in data_sets:\n",
    "        documents.append(combine_relevant_articles(data))\n",
    "        \n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    return [clean(document).split() for document in documents] \n",
    "\n",
    "def clean(doc):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "    doc = ''.join([i if ord(i) < 128 else ' ' for i in doc])\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "    \n",
    "corpuses = derive_corpuses(data_sets)\n",
    "dictionary = corpora.Dictionary(corpuses)\n",
    "doc_term_matrix = [dictionary.doc2bow(corpus) for corpus in corpuses]\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Using Existing Data\n",
    "Before continuing, let's take a look at some of the topics discovered\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00019183, 0.00019183, 0.00019183, 0.00019183, 0.00019183,\n",
       "       0.00019183, 0.00019183, 0.00019183, 0.00019183, 0.00019183],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import pyLDAvis.gensim\n",
    "\n",
    "np.sort(ldamodel.get_topics()[0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.026*\"overdose\" + 0.017*\"county\" + 0.014*\"death\" + 0.013*\"allegheny\" + 0.010*\"treatment\" + 0.009*\"service\" + 0.009*\"drug\" + 0.008*\"opiaterelated\" + 0.007*\"department\" + 0.007*\"2014\" + 0.007*\"naloxone\" + 0.007*\"prescription\" + 0.006*\"var\" + 0.006*\"health\" + 0.006*\"figure\" + 0.005*\"data\" + 0.005*\"use\" + 0.005*\"2016\" + 0.005*\"individual\" + 0.004*\"medication\" + 0.004*\"risk\" + 0.004*\"sud\" + 0.004*\"page\" + 0.004*\"july\" + 0.004*\"opiate\" + 0.004*\"year\" + 0.004*\"police\" + 0.004*\"fatal\" + 0.004*\"analysis\" + 0.004*\"period\"'), (1, u'0.000*\"infection\" + 0.000*\"data\" + 0.000*\"2016\" + 0.000*\"overdose\" + 0.000*\"county\" + 0.000*\"health\" + 0.000*\"sir\" + 0.000*\"asset\" + 0.000*\"treatment\" + 0.000*\"service\" + 0.000*\"allegheny\" + 0.000*\"medical\" + 0.000*\"death\" + 0.000*\"dataset\" + 0.000*\"2014\" + 0.000*\"hospital\" + 0.000*\"oregon\" + 0.000*\"year\" + 0.000*\"national\" + 0.000*\"use\" + 0.000*\"figure\" + 0.000*\"department\" + 0.000*\"public\" + 0.000*\"target\" + 0.000*\"center\" + 0.000*\"requires\" + 0.000*\"people\" + 0.000*\"owner\" + 0.000*\"column\" + 0.000*\"visualization\"'), (2, u'0.034*\"infection\" + 0.019*\"sir\" + 0.017*\"oregon\" + 0.015*\"hospital\" + 0.013*\"target\" + 0.013*\"2016\" + 0.011*\"health\" + 0.011*\"national\" + 0.008*\"hhs\" + 0.008*\"baseline\" + 0.008*\"data\" + 0.008*\"2013\" + 0.007*\"careassociated\" + 0.007*\"surgery\" + 0.007*\"facility\" + 0.007*\"report\" + 0.007*\"met\" + 0.006*\"care\" + 0.006*\"statistically\" + 0.006*\"2014\" + 0.005*\"central\" + 0.005*\"clabsi\" + 0.005*\"bloodstream\" + 0.005*\"cdc\" + 0.005*\"08\" + 0.004*\"following\" + 0.004*\"patient\" + 0.004*\"adult\" + 0.004*\"95\" + 0.004*\"icu\"'), (3, u'0.000*\"overdose\" + 0.000*\"county\" + 0.000*\"health\" + 0.000*\"death\" + 0.000*\"allegheny\" + 0.000*\"data\" + 0.000*\"2016\" + 0.000*\"infection\" + 0.000*\"service\" + 0.000*\"opiaterelated\" + 0.000*\"use\" + 0.000*\"2014\" + 0.000*\"dataset\" + 0.000*\"sir\" + 0.000*\"department\" + 0.000*\"naloxone\" + 0.000*\"drug\" + 0.000*\"var\" + 0.000*\"treatment\" + 0.000*\"hospital\" + 0.000*\"risk\" + 0.000*\"prescription\" + 0.000*\"target\" + 0.000*\"sud\" + 0.000*\"year\" + 0.000*\"figure\" + 0.000*\"oregon\" + 0.000*\"page\" + 0.000*\"medical\" + 0.000*\"national\"'), (4, u'0.015*\"dataset\" + 0.012*\"asset\" + 0.010*\"data\" + 0.008*\"medical\" + 0.007*\"try\" + 0.007*\"column\" + 0.006*\"please\" + 0.006*\"owner\" + 0.006*\"requires\" + 0.005*\"value\" + 0.005*\"visualization\" + 0.005*\"center\" + 0.005*\"dimension\" + 0.004*\"view\" + 0.004*\"chart\" + 0.004*\"alert\" + 0.004*\"contact\" + 0.003*\"use\" + 0.003*\"external\" + 0.003*\"want\" + 0.003*\"see\" + 0.003*\"count\" + 0.003*\"public\" + 0.003*\"row\" + 0.003*\"access\" + 0.003*\"submitted\" + 0.003*\"document\" + 0.003*\"inventory\" + 0.003*\"lens\" + 0.003*\"sure\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=5, num_words=30))\n",
    "# print(ldamodel.top_topics())\n",
    "# for i in ldamodel.print_topics(num_topics=30, num_words=3):\n",
    "#     print(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first examine the topics generated with [the following problem](http://www.allerganclinicaltrials.com/pdfs/neuroscience/Approved/Combunox-OXY-MD-05-00.pdf) as well as validate using the existing JSON attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.61558586), (2, 0.347975), (4, 0.036058474)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_url = \"http://www.allerganclinicaltrials.com/pdfs/neuroscience/Approved/Combunox-OXY-MD-05-00.pdf\"\n",
    "problem_url_file = urllib.urlopen(problem_url).read()\n",
    "pdf = PyPDF2.PdfFileReader(StringIO(problem_url_file))\n",
    "joined_pdf = \"\"\n",
    "for i in range(pdf.getNumPages()):\n",
    "    joined_pdf += pdf.getPage(i).extractText()\n",
    "\n",
    "            \n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "cleaned_problem = clean(joined_pdf).split()\n",
    "\n",
    "ldamodel[dictionary.doc2bow(cleaned_problem)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyLDAvis.gensim",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-bf43c28f5ca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named pyLDAvis.gensim"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
